{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the BigCode Models\n",
    "\n",
    "*Authored by Arjun Guha, with technical assistance from Raymond Li and Carolyn Jane Anderson.*\n",
    "\n",
    "To use this notebook, you will need a recent version of the Transformers library. I don't know what the minimum version is, \n",
    "but version 4.20 does not work, whereas version 4.25 does work.\n",
    "\n",
    "This notebook should run on a GPU with 8GB VRAM. I have not tried it on a CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:31:50.764400Z",
     "iopub.status.busy": "2022-12-21T15:31:50.764145Z",
     "iopub.status.idle": "2022-12-21T15:31:53.894135Z",
     "shell.execute_reply": "2022-12-21T15:31:53.893565Z",
     "shell.execute_reply.started": "2022-12-21T15:31:50.764369Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:31:53.900483Z",
     "iopub.status.busy": "2022-12-21T15:31:53.900070Z",
     "iopub.status.idle": "2022-12-21T15:31:53.902778Z",
     "shell.execute_reply": "2022-12-21T15:31:53.902411Z",
     "shell.execute_reply.started": "2022-12-21T15:31:53.900467Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BigCode models require a little more configuration that off-the-shelf Transformers, which I've included below. I expect some of this won't be necessary in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:33:58.521298Z",
     "iopub.status.busy": "2022-12-21T15:33:58.521020Z",
     "iopub.status.idle": "2022-12-21T15:34:14.146557Z",
     "shell.execute_reply": "2022-12-21T15:34:14.145975Z",
     "shell.execute_reply.started": "2022-12-21T15:33:58.521274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"bigcode/christmas-models\"\n",
    "MODEL_REVISION = \"dedup-alt-comments\"\n",
    "FIM_PREFIX = \"<fim-prefix>\"\n",
    "FIM_MIDDLE = \"<fim-middle>\"\n",
    "FIM_SUFFIX = \"<fim-suffix>\"\n",
    "FIM_PAD = \"<fim-pad>\"\n",
    "ENDOFTEXT = \"<|endoftext|>\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, revision=MODEL_REVISION, trust_remote_code=True).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "\n",
    "# Note that the special tokens must be listed in the order below.\n",
    "tokenizer.add_special_tokens({\n",
    "  \"additional_special_tokens\": [ ENDOFTEXT,FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD ],\n",
    "  \"pad_token\": ENDOFTEXT,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T13:36:31.497735Z",
     "iopub.status.busy": "2022-12-21T13:36:31.497461Z",
     "iopub.status.idle": "2022-12-21T13:36:31.503588Z",
     "shell.execute_reply": "2022-12-21T13:36:31.502414Z",
     "shell.execute_reply.started": "2022-12-21T13:36:31.497716Z"
    }
   },
   "source": [
    "The BigCode models supports fill-in-the-middle (FIM), and the output may include special tokens used for the FIM task. When decoding output,\n",
    "if we just strip them away (e.g., with `skip_special_tokens=True`), then we will get jumbled output. The code below is designed to clean up special\n",
    "tokens in a manner that makes sense for a FIM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:32:14.410863Z",
     "iopub.status.busy": "2022-12-21T15:32:14.410648Z",
     "iopub.status.idle": "2022-12-21T15:32:14.419918Z",
     "shell.execute_reply": "2022-12-21T15:32:14.419368Z",
     "shell.execute_reply.started": "2022-12-21T15:32:14.410845Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_at_first_special_token(output_str):\n",
    "    \"\"\"\n",
    "    Instead, this function clips the output at the first special token that it finds.\n",
    "    \"\"\"\n",
    "    truncate_index = len(output_str)\n",
    "    for special_token in [ FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD, ENDOFTEXT ]:\n",
    "        ix = output_str.find(special_token)\n",
    "        if ix != -1 and ix < truncate_index:\n",
    "            truncate_index = ix\n",
    "    return output_str[:truncate_index]\n",
    "\n",
    "\n",
    "def strip_left_padding(output_tensor):\n",
    "    \"\"\"\n",
    "    Since we are not using skip_special_tokens as described above, when batching results of varying length,\n",
    "    the output will contain <|endoftext|> tokens on the left. This code strips those out.\n",
    "    \"\"\"\n",
    "    start_index = 0\n",
    "    while output_tensor[start_index].item() == tokenizer.pad_token_id:\n",
    "        start_index += 1\n",
    "    return output_tensor[start_index:]\n",
    "\n",
    "\n",
    "def stop_at_stop_token(decoded_string, stop_tokens):\n",
    "    \"\"\"\n",
    "    Produces the prefix of decoded_string that ends at the first occurrence of\n",
    "    a stop_token.\n",
    "    WARNING: the decoded_string *must not* include the prompt, which may have stop tokens\n",
    "    itself.\n",
    "    \"\"\"\n",
    "    min_stop_index = len(decoded_string)\n",
    "    for stop_token in stop_tokens:\n",
    "        stop_index = decoded_string.find(stop_token)\n",
    "        if stop_index != -1 and stop_index < min_stop_index:\n",
    "            min_stop_index = stop_index\n",
    "    return decoded_string[:min_stop_index]\n",
    "\n",
    "\n",
    "def completions(prompts, stop_tokens = [ \"\\n\\n\" ], max_length: int = 128, temperature: float = 0.2, top_p: float = 0.95):\n",
    "    \"\"\"\n",
    "    This function generates completions up to given maximum length or the first stop token. The default stop token\n",
    "    is two newlines, which is usually the boundary between top-level functions in many programming languages.\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(prompts) == str:\n",
    "        prompts = [prompts]\n",
    "\n",
    "    # `.rstrip` is essential. Trailing whitespae produces really bad completions.\n",
    "    prompts = [ p.rstrip() for p in prompts ] \n",
    "    # `return_token_type_ids=False` is essential, or we get nonsense output.\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(DEVICE)\n",
    "    max_length = max_length + inputs.input_ids[0].size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    cleaned_output_strs = [ truncate_at_first_special_token(tokenizer.decode(strip_left_padding(output))) for output in outputs ]\n",
    "    return [\n",
    "        stop_at_stop_token(output_str[len(prompt):], stop_tokens) for (output_str, prompt) in zip(cleaned_output_strs, prompts)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example where we get one completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:51:01.035636Z",
     "iopub.status.busy": "2022-12-21T15:51:01.034964Z",
     "iopub.status.idle": "2022-12-21T15:51:03.389024Z",
     "shell.execute_reply": "2022-12-21T15:51:03.388461Z",
     "shell.execute_reply.started": "2022-12-21T15:51:01.035593Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def foo(n):\n",
      "    \"\"\"Check if n is prime\"\"\"\n",
      "    if n == 1:\n",
      "        return False\n",
      "    for i in range(2, n):\n",
      "        if n % i == 0:\n",
      "            return False\n",
      "    return True\n"
     ]
    }
   ],
   "source": [
    "prompt = 'def foo(n):\\n    \"\"\"Check if n is prime\"\"\"'\n",
    "     \n",
    "print(prompt + completions(prompt)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below is interesting because it has two inputs of different lengths in a batch, so one of them gets padded. Without the postprocessing above, we would get special tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:32:19.345067Z",
     "iopub.status.busy": "2022-12-21T15:32:19.344802Z",
     "iopub.status.idle": "2022-12-21T15:32:21.861635Z",
     "shell.execute_reply": "2022-12-21T15:32:21.861049Z",
     "shell.execute_reply.started": "2022-12-21T15:32:19.345048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "def hello(request):\n",
      "    return HttpResponse(\"Hello, world. You're at the polls index.\")\n",
      "********\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"def hello(\", \"if __name__\"]\n",
    "results = completions(prompts)\n",
    "for (prompt, result) in zip(prompts, results):\n",
    "    print(\"********\")\n",
    "    print(prompt + result)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example uses the model to generate a docstring from a function name. It uses `\"\"\"` as a stop token to terminate at the end of the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:32:25.414061Z",
     "iopub.status.busy": "2022-12-21T15:32:25.413782Z",
     "iopub.status.idle": "2022-12-21T15:32:27.747130Z",
     "shell.execute_reply": "2022-12-21T15:32:27.746499Z",
     "shell.execute_reply.started": "2022-12-21T15:32:25.414040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Factorial of n\n"
     ]
    }
   ],
   "source": [
    "print(completions('def fac(n):\\n    \"\"\"', stop_tokens= [ '\"\"\"' ])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Fill-in-the-Middle (FIM)\n",
    "\n",
    "The BigCode models support fill-in-the-middle or infilling. The `infill` function below takes in a (list of) prefix-suffix tuples, and produces code that goes between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:55:23.111348Z",
     "iopub.status.busy": "2022-12-21T15:55:23.110804Z",
     "iopub.status.idle": "2022-12-21T15:55:23.116326Z",
     "shell.execute_reply": "2022-12-21T15:55:23.115820Z",
     "shell.execute_reply.started": "2022-12-21T15:55:23.111327Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_fim_part(s: str):\n",
    "    # Find the index of <fim-middle>\n",
    "    start = s.find(FIM_MIDDLE) + len(FIM_MIDDLE)\n",
    "    stop = s.find(ENDOFTEXT, start) or len(s)\n",
    "    return s[start:stop]\n",
    "\n",
    "def infill(prefix_suffix_tuples, max_tokens: int = 50, temperature: float = 0.2, top_p : float = 0.95):\n",
    "    if type(prefix_suffix_tuples) == tuple:\n",
    "        prefix_suffix_tuples = [prefix_suffix_tuples]\n",
    "        \n",
    "    prompts = [f\"{FIM_PREFIX}{prefix}{FIM_SUFFIX}{suffix}{FIM_MIDDLE}\" for prefix, suffix in prefix_suffix_tuples]\n",
    "    # `return_token_type_ids=False` is essential, or we get nonsense output.\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(DEVICE)\n",
    "    max_length = inputs.input_ids[0].size(0) + max_tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    # WARNING: cannot use skip_special_tokens, because it blows away the the fim special tokens.\n",
    "    return [        \n",
    "        extract_fim_part(tokenizer.decode(tensor, skip_special_tokens=False)) for tensor in outputs\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we use FIM to fill in the base case of the fibonacci function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:55:45.171987Z",
     "iopub.status.busy": "2022-12-21T15:55:45.171442Z",
     "iopub.status.idle": "2022-12-21T15:55:46.079883Z",
     "shell.execute_reply": "2022-12-21T15:55:46.079368Z",
     "shell.execute_reply.started": "2022-12-21T15:55:45.171967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fib(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    elif n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return fib(n - 2) + fib(n - 1)\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"def fib(n):\"\"\"\n",
    "\n",
    "suffix = \"\"\"    else:\n",
    "        return fib(n - 2) + fib(n - 1)\"\"\"\n",
    "\n",
    "[middle] = infill((prefix, suffix))\n",
    "print(prefix + middle + suffix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2db2a4615b40a0c95efaf4cbb103a2b27bedef6f53cc77a9dc307c8c19c5c96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
