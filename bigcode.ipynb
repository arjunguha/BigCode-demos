{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this notebook, you will need a recent version of the Transformers library. I don't know what the minimum version is, but version 4.20 does not work, whereas version 4.25 does work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T13:32:37.691987Z",
     "iopub.status.busy": "2022-12-21T13:32:37.691776Z",
     "iopub.status.idle": "2022-12-21T13:32:41.284281Z",
     "shell.execute_reply": "2022-12-21T13:32:41.283710Z",
     "shell.execute_reply.started": "2022-12-21T13:32:37.691950Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T13:32:41.290828Z",
     "iopub.status.busy": "2022-12-21T13:32:41.290698Z",
     "iopub.status.idle": "2022-12-21T13:32:41.294086Z",
     "shell.execute_reply": "2022-12-21T13:32:41.293607Z",
     "shell.execute_reply.started": "2022-12-21T13:32:41.290819Z"
    }
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BigCode models require a little more configuration that off-the-shelf Transformers, which I've included below. I expect some of this won't be necessary in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T13:33:39.986040Z",
     "iopub.status.busy": "2022-12-21T13:33:39.985341Z",
     "iopub.status.idle": "2022-12-21T13:35:45.513286Z",
     "shell.execute_reply": "2022-12-21T13:35:45.512648Z",
     "shell.execute_reply.started": "2022-12-21T13:33:39.986013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47741bb8eb0640c29b8788b9292d44b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/948 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf76de4301694c1c856bf7bc52994299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30313fe6c688480bb3c5770189684c35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/15.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5a263f778f4758a457f4c8190ce113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.60G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b20e265d6104d7781a2cf8979e8c3f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/335 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae46e63a8ec4f3689422c722b699fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b673a92c1a4172bb64e40f9cf2f56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"bigcode/christmas-models\"\n",
    "MODEL_REVISION = \"dedup-alt-comments\"\n",
    "FIM_PREFIX = \"<fim-prefix>\"\n",
    "FIM_MIDDLE = \"<fim-middle>\"\n",
    "FIM_SUFFIX = \"<fim-suffix>\"\n",
    "FIM_PAD = \"<fim-pad>\"\n",
    "ENDOFTEXT = \"<|endoftext|>\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, revision=MODEL_REVISION, trust_remote_code=True).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "\n",
    "tokenizer.add_special_tokens({\n",
    "  \"additional_special_tokens\": [ FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD, ENDOFTEXT ],\n",
    "  \"pad_token\": ENDOFTEXT,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T13:36:31.497735Z",
     "iopub.status.busy": "2022-12-21T13:36:31.497461Z",
     "iopub.status.idle": "2022-12-21T13:36:31.503588Z",
     "shell.execute_reply": "2022-12-21T13:36:31.502414Z",
     "shell.execute_reply.started": "2022-12-21T13:36:31.497716Z"
    }
   },
   "source": [
    "The BigCode models supports fill-in-the-middle (FIM), and the output may include special tokens used for the FIM task. When decoding output,\n",
    "if we just strip them away (e.g., with `skip_special_tokens=True`), then we will get jumbled output. The code below is designed to clean up special\n",
    "tokens in a manner that makes sense for a FIM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T13:38:17.476854Z",
     "iopub.status.busy": "2022-12-21T13:38:17.476320Z",
     "iopub.status.idle": "2022-12-21T13:38:17.484306Z",
     "shell.execute_reply": "2022-12-21T13:38:17.483610Z",
     "shell.execute_reply.started": "2022-12-21T13:38:17.476833Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_at_first_special_token(output_str):\n",
    "    \"\"\"\n",
    "    Instead, this function clips the output at the first special token that it finds.\n",
    "    \"\"\"\n",
    "    truncate_index = len(output_str)\n",
    "    for special_token in [ FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_PAD, ENDOFTEXT ]:\n",
    "        ix = output_str.find(special_token)\n",
    "        if ix != -1 and ix < truncate_index:\n",
    "            truncate_index = ix\n",
    "    return output_str[:truncate_index]\n",
    "\n",
    "\n",
    "def strip_left_padding(output_tensor):\n",
    "    \"\"\"\n",
    "    Since we are not using skip_special_tokens as described above, when batching results of varying length,\n",
    "    the output will contain <|endoftext|> tokens on the left. This code strips those out.\n",
    "    \"\"\"\n",
    "    start_index = 0\n",
    "    while output_tensor[start_index].item() == tokenizer.pad_token_id:\n",
    "        start_index += 1\n",
    "    return output_tensor[start_index:]\n",
    "\n",
    "\n",
    "def stop_at_stop_token(decoded_string, stop_tokens):\n",
    "    \"\"\"\n",
    "    Produces the prefix of decoded_string that ends at the first occurrence of\n",
    "    a stop_token.\n",
    "    WARNING: the decoded_string *must not* include the prompt, which may have stop tokens\n",
    "    itself.\n",
    "    \"\"\"\n",
    "    min_stop_index = len(decoded_string)\n",
    "    for stop_token in stop_tokens:\n",
    "        stop_index = decoded_string.find(stop_token)\n",
    "        if stop_index != -1 and stop_index < min_stop_index:\n",
    "            min_stop_index = stop_index\n",
    "    return decoded_string[:min_stop_index]\n",
    "\n",
    "\n",
    "def completions(prompts, stop_tokens = [ \"\\n\\n\" ], max_length: int = 128, temperature: float = 0.2, top_p: float = 0.95):\n",
    "    \"\"\"\n",
    "    This function generates completions up to given maximum length or the first stop token. The default stop token\n",
    "    is two newlines, which is usually the boundary between top-level functions in many programming languages.\n",
    "    \"\"\"\n",
    "    \n",
    "    if type(prompts) == str:\n",
    "        prompts = [prompts]\n",
    "\n",
    "    # `.rstrip` is essential. Trailing whitespae produces really bad completions.\n",
    "    prompts = [ p.rstrip() for p in prompts ] \n",
    "    # `return_token_type_ids=False` is essential, or we get nonsense output.\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(DEVICE)\n",
    "    max_length = max_length + inputs.input_ids[0].size(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    cleaned_output_strs = [ truncate_at_first_special_token(tokenizer.decode(strip_left_padding(output))) for output in outputs ]\n",
    "    return [\n",
    "        stop_at_stop_token(output_str[len(prompt):], stop_tokens) for (output_str, prompt) in zip(cleaned_output_strs, prompts)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple example where we get one completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T13:45:50.749004Z",
     "iopub.status.busy": "2022-12-21T13:45:50.748718Z",
     "iopub.status.idle": "2022-12-21T13:45:53.137871Z",
     "shell.execute_reply": "2022-12-21T13:45:53.137270Z",
     "shell.execute_reply.started": "2022-12-21T13:45:50.748985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def foo(n):\n",
      "    \"\"\"Check if n is prime\"\"\"\n",
      "    if n == 2:\n",
      "        return True\n",
      "    if n == 3:\n",
      "        return True\n",
      "    if n % 2 == 0 or n % 3 == 0:\n",
      "        return False\n",
      "    for i in range(5, int(n ** 0.5) + 1, 6):\n",
      "        if n % i == 0 or n % (i + 2) == 0:\n",
      "            return False\n",
      "    return True\n"
     ]
    }
   ],
   "source": [
    "prompt = 'def foo(n):\\n    \"\"\"Check if n is prime\"\"\"'\n",
    "     \n",
    "print(prompt + completions(prompt)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below is interesting because it has two inputs of different lengths in a batch, so one of them gets padded. Without the postprocessing above, we would get special tokens below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T13:26:18.354748Z",
     "iopub.status.busy": "2022-12-21T13:26:18.354136Z",
     "iopub.status.idle": "2022-12-21T13:26:21.022359Z",
     "shell.execute_reply": "2022-12-21T13:26:21.021751Z",
     "shell.execute_reply.started": "2022-12-21T13:26:18.354724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********\n",
      "def hello(request):\n",
      "    return HttpResponse(\"Hello, world. You're at the polls index.\")\n",
      "********\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"def hello(\", \"if __name__\"]\n",
    "results = completions(prompts)\n",
    "for (prompt, result) in zip(prompts, results):\n",
    "    print(\"********\")\n",
    "    print(prompt + result)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2db2a4615b40a0c95efaf4cbb103a2b27bedef6f53cc77a9dc307c8c19c5c96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
