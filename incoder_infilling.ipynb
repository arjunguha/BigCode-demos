{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infilling with Incoder\n",
    "\n",
    "*Authored by Arjun Guha with some guidance from Carolyn Jane Anderson and Daniel Fried.*\n",
    " \n",
    "This is not a BigCode model, but BigCode does infilling similar to Incoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:31:50.764400Z",
     "iopub.status.busy": "2022-12-21T15:31:50.764145Z",
     "iopub.status.idle": "2022-12-21T15:31:53.894135Z",
     "shell.execute_reply": "2022-12-21T15:31:53.893565Z",
     "shell.execute_reply.started": "2022-12-21T15:31:50.764369Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left-side padding is essential for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:33:58.521298Z",
     "iopub.status.busy": "2022-12-21T15:33:58.521020Z",
     "iopub.status.idle": "2022-12-21T15:34:14.146557Z",
     "shell.execute_reply": "2022-12-21T15:34:14.145975Z",
     "shell.execute_reply.started": "2022-12-21T15:33:58.521274Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "MODEL_NAME = \"facebook/incoder-6B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": \"<pad>\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:55:23.111348Z",
     "iopub.status.busy": "2022-12-21T15:55:23.110804Z",
     "iopub.status.idle": "2022-12-21T15:55:23.116326Z",
     "shell.execute_reply": "2022-12-21T15:55:23.115820Z",
     "shell.execute_reply.started": "2022-12-21T15:55:23.111327Z"
    }
   },
   "outputs": [],
   "source": [
    "def strip_left_padding(output_tensor):\n",
    "    \"\"\"\n",
    "    Since we are not using skip_special_tokens, when batching results of varying length,\n",
    "    the output will contain <pad> tokens on the left. This code strips those out. It also strips out\n",
    "    the <|endoftext|> token that marks the begining of strings.\n",
    "    \"\"\"\n",
    "    start_index = 0\n",
    "    while output_tensor[start_index].item() == tokenizer.pad_token_id or output_tensor[start_index].item() == 2:\n",
    "        start_index += 1\n",
    "    return output_tensor[start_index:]\n",
    "\n",
    "def extract_fim_part(s: str, prompt):\n",
    "    \"\"\"\n",
    "    This skips the prompt and extracts code up to <|endofmask|> or the end of string.\n",
    "    \"\"\"\n",
    "    stop_index = s.find(\"<|endofmask|>\")\n",
    "    if stop_index == -1:\n",
    "        stop_index = len(s)\n",
    "    return s[len(prompt):stop_index]\n",
    "\n",
    "def infill(prefix_suffix_tuples, max_tokens: int = 50, temperature: float = 0.2, top_p : float = 0.95):\n",
    "    if type(prefix_suffix_tuples) == tuple:\n",
    "        prefix_suffix_tuples = [prefix_suffix_tuples]\n",
    "        \n",
    "    prompts = [f\"{prefix}<|mask:0|>{suffix}<|mask:1|><|mask:0|>\" for prefix, suffix in prefix_suffix_tuples]\n",
    "    # `return_token_type_ids=False` is essential, or we get nonsense output.\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(DEVICE)\n",
    "    max_length = inputs.input_ids[0].size(0) + max_tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            do_sample=True,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            max_length=max_length\n",
    "        )\n",
    "    # WARNING: cannot use skip_special_tokens, because it blows away the FIM special tokens.\n",
    "    return [        \n",
    "        extract_fim_part(tokenizer.decode(strip_left_padding(tensor), clean_up_tokenization_spaces=False, skip_special_tokens=False), prompt) \n",
    "        for (tensor, prompt) in zip(outputs, prompts)\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we infill a batch with two problems:\n",
    "1.  The return type of factorial, and\n",
    "2.  A problem from OpenAI HumanEval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-21T15:55:45.171987Z",
     "iopub.status.busy": "2022-12-21T15:55:45.171442Z",
     "iopub.status.idle": "2022-12-21T15:55:46.079883Z",
     "shell.execute_reply": "2022-12-21T15:55:46.079368Z",
     "shell.execute_reply.started": "2022-12-21T15:55:45.171967Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = \"\"\"def fac(n) -> \"\"\"\n",
    "\n",
    "suffix = \"\"\":\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * fac(n - 1)\"\"\"\n",
    "\n",
    "prefix1 =  \"def int_to_mini_roman(number: int) -> str:\\n    \\\"\\\"\\\"\\n    Given a positive integer, obtain its roman numeral equivalent as a string,\\n    and return it in lowercase.\\n    Restrictions: 1 <= num <= 1000\\n\\n    Examples:\\n    >>> int_to_mini_roman(19)\\n    'xix'\\n    >>> int_to_mini_roman(152)\\n    'clii'\\n    >>> int_to_mini_roman(426)\\n    'cdxxvi'\\n    \\\"\\\"\\\"\\n    roman_numerals = {\\n        1000: 'm',\\n        900: 'cm',\\n        500: 'd',\\n        400: 'cd',\\n        100: 'c',\\n        90: 'xc',\\n        50: 'l',\\n        40: 'xl',\\n        10: 'x',\\n        9: 'ix',\\n        5: 'v',\\n        4: 'iv',\\n        1: 'i'\\n    }\\n    roman_numeral = ''\\n    for key in sorted(roman_numerals.keys(), reverse=True):\\n        roman_numeral += roman_numerals[key] * (number // key)\"\n",
    "suffix1 =  \"    return roman_numeral\\n\\n\"\n",
    "\n",
    "[middle, middle1] = infill([(prefix, suffix), (prefix1, suffix1)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is valid Python. But, it is a little repetitive. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def fac(n) -> >>>>>>>>>>>>int:\n",
      "def fac(n: int) -> int:\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * fac(n - 1)\n",
      "    \n",
      "def fac(n: int) -> int:\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * fac(n - 1)\n",
      "    \n",
      "def fac<<<<<<<<<<<<<<<:\n",
      "    if n == 0:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * fac(n - 1)\n"
     ]
    }
   ],
   "source": [
    "print(prefix +  \">>>>>>>>>>>>\" + middle + \"<<<<<<<<<<<<<<<\" + suffix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def int_to_mini_roman(number: int) -> str:\n",
      "    \"\"\"\n",
      "    Given a positive integer, obtain its roman numeral equivalent as a string,\n",
      "    and return it in lowercase.\n",
      "    Restrictions: 1 <= num <= 1000\n",
      "\n",
      "    Examples:\n",
      "    >>> int_to_mini_roman(19)\n",
      "    'xix'\n",
      "    >>> int_to_mini_roman(152)\n",
      "    'clii'\n",
      "    >>> int_to_mini_roman(426)\n",
      "    'cdxxvi'\n",
      "    \"\"\"\n",
      "    roman_numerals = {\n",
      "        1000: 'm',\n",
      "        900: 'cm',\n",
      "        500: 'd',\n",
      "        400: 'cd',\n",
      "        100: 'c',\n",
      "        90: 'xc',\n",
      "        50: 'l',\n",
      "        40: 'xl',\n",
      "        10: 'x',\n",
      "        9: 'ix',\n",
      "        5: 'v',\n",
      "        4: 'iv',\n",
      "        1: 'i'\n",
      "    }\n",
      "    roman_numeral = ''\n",
      "    for key in sorted(roman_numerals.keys(), reverse=True):\n",
      "        roman_numeral += roman_numerals[key] * (number // key)>>>>>>>>>>>>\n",
      "        number %= key\n",
      "<<<<<<<<<<<<<<<    return roman_numeral\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prefix1 +  \">>>>>>>>>>>>\" + middle1 + \"<<<<<<<<<<<<<<<\" + suffix1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2db2a4615b40a0c95efaf4cbb103a2b27bedef6f53cc77a9dc307c8c19c5c96"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
